# An-Effective-Approach-for-Attention-based-Neural-Machine-Translation
Examine the impact of a customized attention layer within a sequence-to-sequence model for enhancing translation quality in NMT systems.
Develop and integrate a novel mechanism to the attention layer to address issues of over-translating or under-translating phrases, thus ensuring that each input word proportionately influences the translated output.
Assess the performance improvements brought by the new attention layer with coverage on a standard NMT dataset, using metrics such as BLEU scores for quantifiable evaluation
# Download Dataset
from https://drive.google.com/file/d/1QI68ysC3nU0v7ggB1y3JzXKWwnTsUPVy/view?usp=sharing
